0. Setup

- install Confluent Platform
- install confluent-hub
- confluent-hub install confluentinc/kafka-connect-datagen:latest


1. Start Confluent Platform locally

confluent local services start

    Optional: partial
        confluent local services schema-registry start


2. Create topics

kafka-topics --create \
  --bootstrap-server localhost:9092 \
  --replication-factor 1 \
  --partitions 4 \
  --config "cleanup.policy=compact" \
  --topic consents

kafka-topics --create \
  --bootstrap-server localhost:9092 \
  --replication-factor 1 \
  --partitions 4 \
  --config "cleanup.policy=compact" \
  --topic contactmails

curl -X POST -H "Content-Type:application/json" -d"{\"schema\":$(jq tostring consent-schema.avsc)}" http://localhost:8081/subjects/consents-value/versions
curl -X POST -H "Content-Type:application/json" -d"{\"schema\":$(jq tostring contactmail-schema.avsc)}" http://localhost:8081/subjects/contactmails-value/versions


3. Generate Data

curl -X POST -H "Content-Type: application/json" \
  --data @connector_config_contactmails.json http://localhost:8083/connectors

curl -X POST -H "Content-Type: application/json" \
  --data @connector_config_consents.json http://localhost:8083/connectors


4. Check data

docker run --network="host" -t edenhill/kafkacat:1.6.0 -b localhost:9092 -t consents -r http://localhost:8081 -s value=avro -C -e
docker run --network="host" -t edenhill/kafkacat:1.6.0 -b localhost:9092 -t contactmails -r http://localhost:8081 -s value=avro -C -e


5. Start ksql locally

LOG_DIR=$CONFLUENT_HOME/ksql_logs ksql

    Optional: via docker
        docker-compose
        docker exec -it ksqldb-cli ksql http://ksqldb-server:8088


6. Inspect topics

show topics;
print 'contactmails' from beginning;
print 'consents' from beginning;


7. "Just join the two topics"

SET 'auto.offset.reset' = 'earliest';

CREATE TABLE contact_mails (key_customerid VARCHAR PRIMARY KEY, contactMailAddress VARCHAR)
    WITH (kafka_topic='contactmails', value_format='avro');

SELECT * FROM contact_mails EMIT CHANGES LIMIT 5;

CREATE STREAM consents (key_customerid VARCHAR KEY)
    WITH (kafka_topic='consents', value_format='avro');

SELECT * FROM consents EMIT CHANGES LIMIT 5;

SELECT consents.key_customerid AS key_customerid,
       consents.customerId,
       consents.hashedCustomerId,
       contact_mails.contactMailAddress AS contactMailAddress,
       consents.consents['company1'] AS consentToCompany1
   FROM consents
   LEFT JOIN contact_mails ON consents.key_customerid = contact_mails.key_customerid
   EMIT CHANGES LIMIT 9;

CREATE STREAM consent_withmails_liketutorial
    WITH (kafka_topic='consentsWithContactMailsLikeTutorial',
          value_format='avro') AS
    SELECT consents.key_customerid AS key_customerid,
           consents.customerId,
           consents.hashedCustomerId,
           contact_mails.contactMailAddress AS contactMailAddress,
           consents.consents['company1'] AS consentToCompany1
    FROM consents
    LEFT JOIN contact_mails ON consents.key_customerid = contact_mails.key_customerid;


//Table to query
CREATE STREAM contact_mails_changelog (key_customerid VARCHAR KEY, contactMailAddress VARCHAR)
    WITH (kafka_topic='contactmails', value_format='avro');

CREATE TABLE contact_mails_materialized AS
    SELECT key_customerid, LATEST_BY_OFFSET(contactMailAddress)
    FROM contact_mails_changelog
    GROUP BY key_customerid
    EMIT CHANGES;

SELECT * FROM contact_mails_materialized WHERE key_customerid='user_9';

    //alternative: dump data to file and search in file


8. Write messages manually

SELECT customerid || ';"' || lower(email) || '"'
FROM users u ;

kafka-topics --create \
  --bootstrap-server localhost:9092 \
  --replication-factor 1 \
  --partitions 4 \
  --config "cleanup.policy=compact" \
  --topic philip.contactmails

docker run --network="host" -v /home/philip/Projects/playground/ksqlDB:/mnt/data -t edenhill/kafkacat:1.6.0 \
-b localhost:9092 -t philip.contactmails -P -K ";" -z lz4 -T -l /mnt/data/contactmails_dump

CREATE TABLE contact_mails_dump (key_customerid VARCHAR PRIMARY KEY, contactMailAddress VARCHAR)
    WITH (kafka_topic='philip.contactmails', value_format='JSON', wrap_single_value=false);

SELECT * FROM contact_mails_dump EMIT CHANGES LIMIT 9;


9. Join consents to dump

SELECT consents.key_customerid AS key_customerid,
       consents.customerId,
       consents.hashedCustomerId,
       contact_mails_dump.contactMailAddress AS contactMailAddress,
       consents.consents['company1'] AS consentToCompany1
   FROM consents
   LEFT JOIN contact_mails_dump ON consents.key_customerid = contact_mails_dump.key_customerid
   EMIT CHANGES LIMIT 100;


10. Co-Partitioned?

docker run --network="host" -t edenhill/kafkacat:1.6.0 \
-b localhost:9092 -t philip.contactmails -C -c1 -f '%k' | hexdump -C

docker run --network="host" -t edenhill/kafkacat:1.6.0 \
-b localhost:9092 -t consents -C -c1 -f '%k' | hexdump -C

ASCII CONVERTER: https://www.rapidtables.com/convert/number/hex-to-ascii.html

docker run --network="host" -t edenhill/kafkacat:1.6.0 \
-b localhost:9092 -t philip.contactmails -C -e -f '%k,%p\n' > partitioning_contactmails.csv

docker run --network="host" -t edenhill/kafkacat:1.6.0 \
-b localhost:9092 -t consents -C -e -f '%k,%p\n' > partitioning_consent.csv

sort -t , -k 1,1 partitioning_contactmails.csv > partitioning_contactmails_sorted.csv
sort -t , -k 1,1 partitioning_consent.csv > partitioning_consent_sorted.csv
join -t , -1 1 -2 1 partitioning_contactmails_sorted.csv partitioning_consent_sorted.csv > partitioning_joined.csv

    via  https://superuser.com/a/26869
    -t ,   : ',' is the field separator
    -k 1,1 : character sort on 1st field
    -1 1   : file 1, 1st field
    -2 1   : file 2, 1st field
    >      : output to file

regex remove lines where not same number twice?

    sed -i.old -r '/^.*?,(0,0|1,1|2,2|3,3|4,4|5,5|6,6|7,7|8,8|9,9|10,10)$/d' partitioning_joined.csv

    via https://stackoverflow.com/a/10822959


11. Write mails manually 2

kafka-topics --create \
  --bootstrap-server localhost:9092 \
  --replication-factor 1 \
  --partitions 4 \
  --config "cleanup.policy=compact" \
  --topic philip.contactmailsWithCorrectPartitions

docker run --network="host" -v /home/philip/Projects/playground/ksqlDB:/mnt/data -t edenhill/kafkacat:1.6.0 \
-X topic.partitioner=murmur2_random \
-b localhost:9092 -t philip.contactmailsWithCorrectPartitions -P -K ";" -z lz4 -T -l /mnt/data/contactmails_dump

docker run --network="host" -t edenhill/kafkacat:1.6.0 \
-b localhost:9092 -t philip.contactmailsWithCorrectPartitions -C -e -f 'Topic %t[%p], offset: %o, key: %k\n'

CREATE TABLE contact_mails_dump_correct (key_customerid VARCHAR PRIMARY KEY, contactMailAddress VARCHAR)
    WITH (kafka_topic='philip.contactmailsWithCorrectPartitions', value_format='JSON', wrap_single_value=false);

SELECT consents.key_customerid AS key_customerid,
       consents.customerId,
       consents.hashedCustomerId,
       contact_mails_dump_correct.contactMailAddress AS contactMailAddress,
       consents.consents['company1'] AS consentToCompany1
   FROM consents
   LEFT JOIN contact_mails_dump_correct ON consents.key_customerid = contact_mails_dump_correct.key_customerid
   EMIT CHANGES LIMIT 100;


13. Messing with time

SELECT customerid || ';{"contactEmailAddress":"' || lower(email) || '","artificialMessageTime":"2000-01-01 01:00:00"}'
FROM users u ;

kafka-topics --create \
  --bootstrap-server localhost:9092 \
  --replication-factor 1 \
  --partitions 4 \
  --config "cleanup.policy=compact" \
  --topic philip.contactmailsWithTime

docker run --network="host" -v /home/philip/Projects/playground/ksqlDB:/mnt/data -t edenhill/kafkacat:1.6.0 \
-X topic.partitioner=murmur2_random \
-b localhost:9092 -t philip.contactmailsWithTime -P -K ";" -z lz4 -T -l /mnt/data/contactmails_dump_with_time

CREATE TABLE contact_mails_dump_oldtimestamp (key_customerid VARCHAR PRIMARY KEY, contactMailAddress VARCHAR, artificialMessageTime VARCHAR)
    WITH (kafka_topic='philip.contactmailsWithTime', value_format='JSON', timestamp='artificialMessageTime', timestamp_format='yyyy-MM-dd HH:mm:ss');

SELECT rowtime, TIMESTAMPTOSTRING(rowtime, 'yyyy-MM-dd HH:mm:ss') AS rowtime_formatted, * FROM contact_mails_dump_oldtimestamp EMIT CHANGES LIMIT 9;

SELECT rowtime, TIMESTAMPTOSTRING(rowtime, 'yyyy-MM-dd HH:mm:ss') AS rowtime_formatted, * FROM consents EMIT CHANGES;


SELECT consents.key_customerid AS key_customerid,
       consents.customerId,
       consents.hashedCustomerId,
       contact_mails_dump_oldtimestamp.contactMailAddress AS contactMailAddress,
       consents.consents['company1'] AS consentToCompany1
   FROM consents
   LEFT JOIN contact_mails_dump_oldtimestamp ON consents.key_customerid = contact_mails_dump_oldtimestamp.key_customerid
   EMIT CHANGES LIMIT 100;


14. Table-table join

CREATE TABLE consents_table (key_customerid VARCHAR PRIMARY KEY)
    WITH (kafka_topic='consents', value_format='avro');

SELECT consents_table.key_customerid AS key_customerid,
       consents_table.customerId,
       consents_table.hashedCustomerId,
       contact_mails_dump_correct.contactMailAddress,
       consents_table.consents['company1'] AS consentToCompany1
   FROM consents_table
   JOIN contact_mails_dump_correct ON consents_table.key_customerid = contact_mails_dump_correct.key_customerid
   EMIT CHANGES;



------------------

CLEANUP

confluent local destroy

docker-compose down
    if necessary:
        docker-compose down  # Stop container on current dir if there is a docker-compose.yml
        docker rm -fv $(docker ps -aq)  # Remove all containers
        sudo lsof -i -P -n | grep <port number>  # List who's using the port
        kill
